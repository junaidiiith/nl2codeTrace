{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junaid/Anaconda/anaconda3/envs/ML/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from api_models import set_llm_and_embed\n",
    "set_llm_and_embed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "base_dir = 'data_repos/ftlr/datasets/eTour'\n",
    "all_code_files_path = os.path.join(base_dir, 'all_code_filenames.txt')\n",
    "all_code_files = [f_name.strip() for f_name in open(all_code_files_path)]\n",
    "\n",
    "all_req_files_path = os.path.join(base_dir, 'all_req_filenames.txt')\n",
    "\n",
    "\n",
    "all_req_contents = {f_name.strip(): open(os.path.join(base_dir, 'req', f_name.strip())).read() for f_name in open(all_req_files_path)}\n",
    "# print(all_req_contents[list(all_req_contents.keys())[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccccbca0155348ce88a3dfb5f842bcd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import javalang\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "FILE_NAME_LABEL = \"File Name\"\n",
    "CLASS_NAME_LABEL = \"Class Name\"\n",
    "DOCSTRING_LABEL = \"Docstring\"\n",
    "ATTRIBUTES_LABEL = \"Attributes\"\n",
    "ATTRIBUTE_NAME_LABEL = \"Attribute Name\"\n",
    "ATTRIBUTES_TYPE_LABEL = \"Attribute Type\"\n",
    "METHODS_LABEL = \"Methods\"\n",
    "METHOD_NAME_LABEL = \"Method Name\"\n",
    "METHOD_SIGNATURE_LABEL = \"Signature\"\n",
    "METHOD_DOCSTRING = \"Method Docstring\"\n",
    "METHOD_PARAMETERS_LABEL = \"Method Parameters\"\n",
    "METHOD_RETURN_LABEL = \"Method Return\"\n",
    "PARAM_NAME_LABEL = \"Parameter Name\"\n",
    "PARAM_TYPE_LABEL = \"Parameter Type\"\n",
    "PARAM_DESCRIPTION_LABEL = \"Description\"\n",
    "CALLS_LABEL = \"calls\"\n",
    "CALLED_BY_LABEL = \"called_by\"\n",
    "\n",
    "\n",
    "def parse_java_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Parse the file into an AST\n",
    "    try:\n",
    "        tree = javalang.parse.parse(content)\n",
    "    except javalang.parser.JavaSyntaxError as e:\n",
    "        # raise e\n",
    "        return {\n",
    "            FILE_NAME_LABEL: file_path.split('/')[-1],\n",
    "            CLASS_NAME_LABEL: file_path.split('/')[-1].split('.')[0],\n",
    "        }\n",
    "        \n",
    "\n",
    "    # Find the main class declaration\n",
    "    class_decl = next(\n",
    "        (type_decl for type_decl in tree.types \\\n",
    "         if isinstance(type_decl, javalang.tree.ClassDeclaration) or \\\n",
    "            isinstance(type_decl, javalang.tree.InterfaceDeclaration)), None\n",
    "    )\n",
    "\n",
    "    class_info = {\n",
    "        CLASS_NAME_LABEL: class_decl.name,\n",
    "        DOCSTRING_LABEL: class_decl.documentation if class_decl.documentation else \"\",\n",
    "        ATTRIBUTES_LABEL: [],\n",
    "        METHODS_LABEL: [],\n",
    "        FILE_NAME_LABEL: file_path.split('/')[-1]\n",
    "    }\n",
    "\n",
    "    # Extract attributes and methods\n",
    "    for _, node in class_decl.filter(javalang.tree.FieldDeclaration):\n",
    "        for field in node.declarators:\n",
    "            class_info[ATTRIBUTES_LABEL].append({\n",
    "                ATTRIBUTE_NAME_LABEL: field.name,\n",
    "                ATTRIBUTES_TYPE_LABEL: node.type.name,\n",
    "            })\n",
    "\n",
    "    for _, node in class_decl.filter(javalang.tree.MethodDeclaration):\n",
    "        method_info = {\n",
    "            METHOD_NAME_LABEL: node.name,\n",
    "            METHOD_DOCSTRING: node.documentation if node.documentation else None,\n",
    "            METHOD_PARAMETERS_LABEL: [],\n",
    "            METHOD_RETURN_LABEL: node.return_type.name if node.return_type else \"void\"\n",
    "        }\n",
    "        for param in node.parameters:\n",
    "            method_info[METHOD_PARAMETERS_LABEL].append({\n",
    "                PARAM_NAME_LABEL: param.name,\n",
    "                PARAM_TYPE_LABEL: param.type.name,\n",
    "            })\n",
    "        class_info[METHODS_LABEL].append(method_info)\n",
    "\n",
    "    return class_info\n",
    "\n",
    "class_info_objects = list()\n",
    "for f_name in tqdm(all_code_files):\n",
    "    class_info = parse_java_file(os.path.join(base_dir, 'code', f_name))\n",
    "    class_info_objects.append(class_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in the graph: 1060\n"
     ]
    }
   ],
   "source": [
    "def get_graph_nodes(class_info_objects):\n",
    "    graph_nodes = dict()\n",
    "    for class_info in class_info_objects:\n",
    "        node = {\n",
    "            CLASS_NAME_LABEL: class_info[CLASS_NAME_LABEL],\n",
    "            FILE_NAME_LABEL: class_info[FILE_NAME_LABEL],\n",
    "            \"type\": \"Class\"\n",
    "        }\n",
    "        if DOCSTRING_LABEL in class_info:\n",
    "            node[DOCSTRING_LABEL] = class_info[DOCSTRING_LABEL]\n",
    "        if ATTRIBUTES_LABEL in class_info:\n",
    "            node[ATTRIBUTES_LABEL] = class_info[ATTRIBUTES_LABEL]\n",
    "        graph_nodes[class_info[CLASS_NAME_LABEL]] = node\n",
    "\n",
    "        class_name = class_info[CLASS_NAME_LABEL]\n",
    "        if METHODS_LABEL not in class_info:\n",
    "            continue\n",
    "        for method_info in class_info[METHODS_LABEL]:\n",
    "            method_name = method_info[METHOD_NAME_LABEL]\n",
    "            method_key = f'{class_name}.{method_name}'\n",
    "            params_str = f\"({','.join([param[PARAM_TYPE_LABEL] for param in method_info[METHOD_PARAMETERS_LABEL]])})\"\n",
    "            method_key_str = f'{method_key}{params_str}'\n",
    "\n",
    "            node = {\n",
    "                CLASS_NAME_LABEL: class_name,\n",
    "                FILE_NAME_LABEL: class_info[FILE_NAME_LABEL],\n",
    "                METHOD_NAME_LABEL: method_info[METHOD_NAME_LABEL],\n",
    "                \"type\": \"Method\",\n",
    "                METHOD_SIGNATURE_LABEL: method_key_str\n",
    "            }\n",
    "\n",
    "            if METHOD_DOCSTRING in method_info:\n",
    "                node[METHOD_DOCSTRING] = method_info[METHOD_DOCSTRING]\n",
    "            \n",
    "            graph_nodes[method_key_str] = node\n",
    "\n",
    "    print(\"Number of nodes in the graph:\", len(graph_nodes))\n",
    "    return graph_nodes\n",
    "\n",
    "graph_nodes = get_graph_nodes(class_info_objects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Present: 1704, Absent: 0\n",
      "Number of nodes in the graph: 1143\n"
     ]
    }
   ],
   "source": [
    "def add_method_calls(graph_nodes, cdg_file_path):\n",
    "    cdg = json.load(open(cdg_file_path))\n",
    "    present, absent = 0, 0\n",
    "    for node in list(graph_nodes.values()):\n",
    "        if node[\"type\"] == \"Method\":\n",
    "            method_key = node[METHOD_SIGNATURE_LABEL]\n",
    "            if method_key in cdg:\n",
    "                graph_nodes[method_key][CALLS_LABEL] = cdg[method_key][\"calls\"]\n",
    "                graph_nodes[method_key][CALLED_BY_LABEL] = cdg[method_key][\"called_by\"]\n",
    "\n",
    "                for node_call in cdg[method_key][\"calls\"] + cdg[method_key][\"called_by\"]:\n",
    "                    if node_call not in cdg:\n",
    "                        absent += 1\n",
    "                        print(f\"Node {node_call} not found\")\n",
    "                    else:\n",
    "                        present += 1\n",
    "                        cdg_call_node = cdg[node_call]\n",
    "                        if node_call not in graph_nodes:\n",
    "                            graph_nodes[node_call] = {\n",
    "                                \"type\": \"Method\",\n",
    "                                METHOD_SIGNATURE_LABEL: node_call,\n",
    "                                CLASS_NAME_LABEL: cdg_call_node['class_name'],\n",
    "                                METHOD_NAME_LABEL: cdg_call_node['method_name'],\n",
    "                                FILE_NAME_LABEL: node[FILE_NAME_LABEL],\n",
    "                            }\n",
    "                        graph_nodes[node_call][CALLED_BY_LABEL] = cdg_call_node[\"called_by\"]\n",
    "                        graph_nodes[node_call][CALLS_LABEL] = cdg_call_node[\"calls\"]\n",
    "\n",
    "    print(f\"Present: {present}, Absent: {absent}\")\n",
    "    print(\"Number of nodes in the graph:\", len(graph_nodes))\n",
    "\n",
    "cdg_path = os.path.join(base_dir, 'etour_method_callgraph.json')\n",
    "add_method_calls(graph_nodes, cdg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "997c5b6380ca4d319775660059ee9805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating Documents:   0%|          | 0/1060 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def create_class_node_doc(graph_node):\n",
    "    content = f\"{graph_node['type']} Name: \" + graph_node[CLASS_NAME_LABEL] + \"\\n\"\n",
    "    \n",
    "    if ATTRIBUTES_LABEL in graph_node and len(graph_node[ATTRIBUTES_LABEL]):\n",
    "        content += f\"Attributes: \\n\"\n",
    "        for attr in graph_node[ATTRIBUTES_LABEL]:\n",
    "            content += f\"{attr[ATTRIBUTE_NAME_LABEL]}: {attr[ATTRIBUTES_TYPE_LABEL]}\\n\"\n",
    "    \n",
    "    if DOCSTRING_LABEL in graph_node:\n",
    "        content += f\"\\n{graph_node[DOCSTRING_LABEL]}\\n\"\n",
    "    \n",
    "    doc = Document(\n",
    "        text=content,\n",
    "        metadata = {\n",
    "            FILE_NAME_LABEL: graph_node[FILE_NAME_LABEL],\n",
    "            \"type\": \"Class\"\n",
    "        },\n",
    "        excluded_embed_metadata_keys=[\"type\"],\n",
    "        excluded_llm_metadata_keys=[\"type\"]\n",
    "    )\n",
    "    return doc\n",
    "\n",
    "\n",
    "def create_method_node_doc(graph_node, show_calls=False):\n",
    "    content = f\"Class Name: {graph_node[CLASS_NAME_LABEL]}\\n\"\n",
    "    content += f\"{graph_node['type']} Name: {graph_node[METHOD_NAME_LABEL]}\\n\"\n",
    "    content += f\"Signature: {graph_node[METHOD_SIGNATURE_LABEL]}\\n\"\n",
    "    \n",
    "    \n",
    "    if METHOD_DOCSTRING in graph_node:\n",
    "        content += f\"\\n{graph_node[METHOD_DOCSTRING]}\\n\"\n",
    "    \n",
    "    if show_calls:\n",
    "        if CALLS_LABEL in graph_node:\n",
    "            content += f\"\\nCalls: \\n\"\n",
    "            for call in graph_node[CALLS_LABEL]:\n",
    "                content += f\"{call}\\n\"\n",
    "        \n",
    "        if CALLED_BY_LABEL in graph_node:\n",
    "            content += f\"\\nCalled By: \\n\"\n",
    "            for called_by in graph_node[CALLED_BY_LABEL]:\n",
    "                content += f\"{called_by}\\n\"\n",
    "    \n",
    "    doc = Document(\n",
    "        text=content,\n",
    "        metadata = {\n",
    "            FILE_NAME_LABEL: graph_node[FILE_NAME_LABEL],\n",
    "            CALLS_LABEL: \", \".join(graph_node[CALLS_LABEL]) if CALLS_LABEL in graph_node else None,\n",
    "            CALLED_BY_LABEL: \", \".join(graph_node[CALLED_BY_LABEL]) if CALLED_BY_LABEL in graph_node else None,\n",
    "            \"type\": \"Method\",\n",
    "            METHOD_SIGNATURE_LABEL: graph_node[METHOD_SIGNATURE_LABEL],\n",
    "        },\n",
    "        excluded_embed_metadata_keys=[METHOD_SIGNATURE_LABEL, CALLS_LABEL, CALLED_BY_LABEL, \"type\"],\n",
    "        excluded_llm_metadata_keys=[METHOD_SIGNATURE_LABEL, CALLS_LABEL, CALLED_BY_LABEL, \"type\"],\n",
    "    )\n",
    "    return doc\n",
    "\n",
    "docs = [\n",
    "    create_class_node_doc(graph_node) \\\n",
    "    if graph_node[\"type\"] == \"Class\" else create_method_node_doc(graph_node) \\\n",
    "    for graph_node in tqdm(graph_nodes.values(), desc=\"Creating Documents\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices import KnowledgeGraphIndex\n",
    "import kuzu\n",
    "from llama_index.graph_stores.kuzu import KuzuGraphStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "\n",
    "def get_kuzu_graph_store(collection_name):\n",
    "    db = kuzu.Database(collection_name)\n",
    "    graph_store = KuzuGraphStore(db)\n",
    "\n",
    "    storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "    return storage_context\n",
    "\n",
    "def create_kg_index(docs, storage_context=None):\n",
    "    class_docs = [doc for doc in docs if doc.metadata[\"type\"] == \"Class\"]\n",
    "    method_docs = [doc for doc in docs if doc.metadata[\"type\"] == \"Method\"]\n",
    "\n",
    "    print(\"Number of Class Nodes:\", len(class_docs))\n",
    "    index = KnowledgeGraphIndex.from_documents(\n",
    "        class_docs,\n",
    "        max_triplets_per_chunk=5,\n",
    "        show_progress=True,\n",
    "        storage_context=storage_context\n",
    "    )\n",
    "\n",
    "    for doc in tqdm(method_docs, desc=\"Adding Method Node triples\"):\n",
    "        calls = doc.metadata.get(CALLS_LABEL, [])\n",
    "        called_by = doc.metadata.get(CALLED_BY_LABEL, [])\n",
    "\n",
    "        calls = calls.split(\", \") if isinstance(calls, str) else []\n",
    "        called_by = called_by.split(\", \") if isinstance(called_by, str) else []\n",
    "        for call in calls:\n",
    "            triple = (doc.metadata[METHOD_SIGNATURE_LABEL], \"calls\", call)\n",
    "            index.upsert_triplet_and_node(triple, doc)\n",
    "        for called_by_node in called_by:\n",
    "            triple = (doc.metadata[METHOD_SIGNATURE_LABEL], \"called_by\", called_by_node)\n",
    "            index.upsert_triplet_and_node(triple, doc)\n",
    "    return index\n",
    "\n",
    "kuzu_store = get_kuzu_graph_store(\"etour_kg\")\n",
    "kg_index = create_kg_index(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indexing.utils import get_parser\n",
    "parser = get_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3726a2f745740fe9f894be3b437a1fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating Requirement Docs:   0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code_nodes = code_parser.create_nodes_with_fltr_setting(use_docstring=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "\n",
    "def get_vector_storage_context(chroma_db_path, collection_name):\n",
    "    db = chromadb.PersistentClient(path=f\"{chroma_db_path}\")\n",
    "    chroma_collection = db.get_or_create_collection(collection_name)\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    return storage_context, vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = base_dir.split('/')[-1]\n",
    "indices_dir = 'indices'\n",
    "storage_context, vector_store = get_vector_storage_context(\n",
    "    f\"{indices_dir}/{dataset_name}\", f\"{dataset_name}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indexing.utils import run_pipeline_multithreaded\n",
    "\n",
    "# transformations = get_transformations(\n",
    "#     llm=llm,\n",
    "#     summary_extractor=True,\n",
    "#     summary_template=summary_template,\n",
    "# )\n",
    "\n",
    "nodes = run_pipeline_multithreaded(docs, num_threads=4, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indexing.indices import create_vector_index\n",
    "\n",
    "indexed_nodes, vector_index = create_vector_index(\n",
    "    nodes=docs[:1], \n",
    "    storage_context=storage_context, \n",
    "    num_threads=8,\n",
    "    pickle_dir=f'indices/embedded_{dataset_name}',\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "vector_index = pickle.load(open('vector_index.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_index = pickle.load(open('kg_index.pkl', 'rb'))\n",
    "kg_qe = kg_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_qe = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_index= ''\n",
    "ki_qe = keyword_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrievers import custom_query_engine\n",
    "\n",
    "custom_kg_qe = custom_query_engine(vector_index, kg_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_ki_qe = custom_query_engine(vector_index, keyword_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engines = {\n",
    "    # \"Vector Index\": vi_qe,\n",
    "    \"Knowledge Graph Index\": kg_qe,\n",
    "    \"Keyword Index\": ki_qe,\n",
    "    # \"Custom KG\": custom_kg_qe,\n",
    "    # \"Custom Keyword\": custom_ki_qe\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('keyword_index.pkl', 'wb') as f:\n",
    "    pickle.dump(keyword_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_template = \\\n",
    "\"\"\"\n",
    "What are the names of the classes that are related to the following use case?\n",
    "{requirement}\n",
    "\n",
    "Provide the answer in a list format and provide ONLY the list of class names as a JSON list.\n",
    "[<\"Class 1 Name\">, <\"Class 2 Name\">, ... <\"Class N Name\">] where N can be up to 10.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0797eaca635245f9ad11dd0688b6e44e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating Requirement Docs:   0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RequirementsNodesCreator:\n",
    "    def __init__(self, base_dir: str, req_contents: dict[str, str]):\n",
    "        self.base_dir = base_dir\n",
    "        self.req_contents = req_contents\n",
    "        \n",
    "\n",
    "    def create_nodes(self):\n",
    "        docs = list()\n",
    "        for file_name, content in tqdm(self.req_contents.items(), desc=\"Creating Requirement Docs\"):\n",
    "            doc = Document(\n",
    "                text=content,\n",
    "                metadata={\n",
    "                    \"file_name\": file_name\n",
    "                }\n",
    "            )\n",
    "            docs += [doc]\n",
    "        \n",
    "        return docs\n",
    "\n",
    "req_parser = RequirementsNodesCreator(base_dir, all_req_contents)\n",
    "req_nodes = req_parser.create_nodes()\n",
    "len(req_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import List\n",
    "from llama_index.core.query_engine import BaseQueryEngine\n",
    "\n",
    "def query_parallel(\n",
    "        query_engine: BaseQueryEngine,\n",
    "        query_template: str, \n",
    "        req_nodes: List[Document], \n",
    "        num_threads=8\n",
    "    ):\n",
    "    progress_bar = tqdm(total=len(req_nodes), desc=\"Processing\", unit=\"Requirement\")\n",
    "    futures = list()\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        for req_node in req_nodes:\n",
    "            future = executor.submit(\n",
    "                query_engine.query,\n",
    "                query_template.format(requirement=req_node.text)\n",
    "            )\n",
    "            futures.append((req_node.metadata[\"file_name\"], future))\n",
    "\n",
    "        results = list()\n",
    "        for file_name, future in futures:\n",
    "            results.append((file_name, future.result()))\n",
    "            progress_bar.update(1)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UC14.txt Empty Response\n",
      "Some results are invalid\n"
     ]
    }
   ],
   "source": [
    "def get_post_processing_results(req_results):\n",
    "    ok = True\n",
    "    llm_results = defaultdict(set)\n",
    "    for file_name, result in req_results:\n",
    "        try:\n",
    "            class_names_list = json.loads(result.response)\n",
    "            for class_name in class_names_list:\n",
    "                llm_results[file_name].add(class_name)\n",
    "        except Exception as e:\n",
    "            print(file_name, result.response)\n",
    "            ok = False\n",
    "\n",
    "    for k, value in llm_results.items():\n",
    "        llm_results[k] = list(value)\n",
    "    \n",
    "    if not ok:\n",
    "        print(\"Some results are invalid\")\n",
    "    else:\n",
    "        print(\"All results are valid\")\n",
    "\n",
    "    return llm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# responses = list()\n",
    "# for req_node in tqdm(req_nodes, desc=\"Querying Requirements\"):\n",
    "#     query = query_template.format(requirement=req_node.text)\n",
    "#     results = query_engine.query(query)\n",
    "    \n",
    "#     print(f\"Query: {query}\")\n",
    "#     print(f\"Results: {results}\")\n",
    "#     print(\"=========================================\")\n",
    "#     responses.append((query, results))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_solutions(file_name):\n",
    "    gts = [line for line in open(file_name).read().split('\\n') if line]\n",
    "    solutions = defaultdict(list)\n",
    "    for gt in gts:\n",
    "        gt_split = gt.split(': ')\n",
    "        file_name = gt_split[0]\n",
    "        class_name = gt_split[1].split('.java')[0]\n",
    "        solutions[file_name].append(class_name)\n",
    "    return solutions\n",
    "\n",
    "solutions = get_solutions('data_repos/ftlr/datasets/eTour/etour_solution_links_english.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_solutions(solutions, llm_results, result_file_name='results.json'):\n",
    "    results = list()\n",
    "    tp, fp = 0, 0\n",
    "    tn, fn = 0, 0\n",
    "    \n",
    "    for file_name, classes in solutions.items():\n",
    "        if file_name in llm_results:\n",
    "            tp += len(set(classes).intersection(set(llm_results[file_name])))\n",
    "            fp += len(set(llm_results[file_name]) - set(classes))\n",
    "            fn += len(set(classes) - set(llm_results[file_name]))\n",
    "            \n",
    "            result = {\n",
    "                \"file_name\": file_name,\n",
    "                \"expected_classes\": sorted(classes),\n",
    "                \"llm_classes\": sorted(llm_results[file_name])\n",
    "            }\n",
    "            results.append(result)\n",
    "    with open(result_file_name, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    \n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"F1 Score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config, query_engine in query_engines.items():\n",
    "    print(f\"Evaluating for {config}\")\n",
    "    req_results = query_parallel(\n",
    "        query_engine, \n",
    "        query_template, \n",
    "        req_nodes, \n",
    "        num_threads=8\n",
    "    )\n",
    "    llm_results = get_post_processing_results(req_results)\n",
    "    print(f\"Results for {config}\")\n",
    "    compare_solutions(solutions, llm_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
