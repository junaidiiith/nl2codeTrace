{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from parameters import parse_args\n",
    "import sys; sys.argv=['']; del sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from constants import (\n",
    "    LLMsMap,\n",
    "    EmbeddingModelsMap,\n",
    ")\n",
    "\n",
    "from api_models import set_llm_and_embed\n",
    "args = parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LLM: meta-llama/Meta-Llama-3-70B-Instruct\n",
      "Using Embedding Model: BAAI/bge-m3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junaid/Anaconda/anaconda3/envs/ML/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/junaid/Anaconda/anaconda3/envs/ML/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "args.embed_model = 'bge_m3'\n",
    "\n",
    "llm_name = LLMsMap[args.llm]\n",
    "embed_model_name = EmbeddingModelsMap[args.embed_model]\n",
    "print(f\"Using LLM: {llm_name}\")\n",
    "print(f\"Using Embedding Model: {embed_model_name}\")\n",
    "\n",
    "set_llm_and_embed(\n",
    "    llm_type=args.llm_type,\n",
    "    llm_name=llm_name,\n",
    "    embed_model_name=embed_model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def get_dataset_nodes(\n",
    "    dataset_name,\n",
    "    path: str = 'indexed_nodes',\n",
    "    use_mc: bool = False,\n",
    "    use_summary: bool = False\n",
    "):\n",
    "    indexed_nodes = pickle.load(open(f'{path}/{dataset_name}{\"_mc\" if use_mc else \"\"}{\"_sm\" if use_summary else \"\"}.pkl', 'rb'))\n",
    "    print(f\"Number of indexed nodes: {len(indexed_nodes)}\")\n",
    "    req_nodes = pickle.load(open(f'similar_requirements/{dataset_name}.pkl', 'rb'))\n",
    "    print(f\"Number of requirements nodes: {len(req_nodes)}\")\n",
    "    return indexed_nodes, req_nodes\n",
    "\n",
    "# indexed_nodes, req_nodes = get_dataset_nodes(dataset_name, 'indexed_nodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.evaluation import SemanticSimilarityEvaluator\n",
    "from llama_index.core import Settings\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.indices import VectorStoreIndex\n",
    "from collections import defaultdict\n",
    "from code2graph import CLASS_NAME_LABEL, get_docs_nxg\n",
    "from typing import List\n",
    "from typing import Union\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.schema import NodeWithScore, Document\n",
    "from constants import BM25_INDEX_RETREIVER, VECTOR_INDEX_RETREIVER\n",
    "from evaluation import evaluate_response, get_solutions\n",
    "from indexing.constants import (\n",
    "    CLASS_NAME_LABEL,\n",
    "    DOCSTRING_LABEL,\n",
    "    ATTRIBUTES_LABEL,\n",
    "    ATTRIBUTE_NAME_LABEL,\n",
    "    ATTRIBUTES_TYPE_LABEL,\n",
    "    METHOD_NAME_LABEL,\n",
    "    METHODS_LABEL,\n",
    ")\n",
    "from llama_index.core import QueryBundle\n",
    "from retrievers import get_reachable_nodes\n",
    "\n",
    "\n",
    "def get_dataset_nodes(\n",
    "    dataset_name,\n",
    "    path: str = 'indexed_nodes',\n",
    "    use_mc: bool = False,\n",
    "    use_summary: bool = False\n",
    "):\n",
    "    indexed_nodes = pickle.load(open(f'{path}/{dataset_name}{\"_mc\" if use_mc else \"\"}{\"_sm\" if use_summary else \"\"}.pkl', 'rb'))\n",
    "    print(f\"Number of indexed nodes: {len(indexed_nodes)}\")\n",
    "    req_nodes = pickle.load(open(f'similar_requirements/{dataset_name}.pkl', 'rb'))\n",
    "    print(f\"Number of requirements nodes: {len(req_nodes)}\")\n",
    "    return indexed_nodes, req_nodes\n",
    "\n",
    "def get_graph_node_str(graph_node):\n",
    "    content = f\"Class {graph_node[CLASS_NAME_LABEL]}\\n\"\n",
    "    if graph_node[DOCSTRING_LABEL]:\n",
    "        content += f\"Docstring: {graph_node[DOCSTRING_LABEL]}\\n\"\n",
    "        \n",
    "    # if ATTRIBUTES_LABEL in graph_node and len(graph_node[ATTRIBUTES_LABEL]):\n",
    "    #     content += f\"Attributes: \\n\"\n",
    "    #     for attr in graph_node[ATTRIBUTES_LABEL]:\n",
    "    #         content += f\"{attr[ATTRIBUTE_NAME_LABEL]}: {attr[ATTRIBUTES_TYPE_LABEL]}\\n\"\n",
    "    \n",
    "    # method_names = [i[METHOD_NAME_LABEL] for i in graph_node[METHODS_LABEL]] \n",
    "    # if len(method_names):\n",
    "    #     content += f\"Methods: {', '.join(method_names)}\"\n",
    "    return content\n",
    "\n",
    "\n",
    "def get_graph_node_str(graph_node):\n",
    "    content = f\"Class {graph_node[CLASS_NAME_LABEL]}\\n\"\n",
    "    if graph_node[DOCSTRING_LABEL]:\n",
    "        content += f\"Docstring: {graph_node[DOCSTRING_LABEL]}\\n\"\n",
    "        \n",
    "    # if ATTRIBUTES_LABEL in graph_node and len(graph_node[ATTRIBUTES_LABEL]):\n",
    "    #     content += f\"Attributes: \\n\"\n",
    "    #     for attr in graph_node[ATTRIBUTES_LABEL]:\n",
    "    #         content += f\"{attr[ATTRIBUTE_NAME_LABEL]}: {attr[ATTRIBUTES_TYPE_LABEL]}\\n\"\n",
    "    \n",
    "    # method_names = [i[METHOD_NAME_LABEL] for i in graph_node[METHODS_LABEL]] \n",
    "    # if len(method_names):\n",
    "    #     content += f\"Methods: {', '.join(method_names)}\"\n",
    "    return content\n",
    "\n",
    "\n",
    "\n",
    "class NL2CodeTracer(BaseRetriever):\n",
    "    def __init__(\n",
    "        self, \n",
    "        dataset_name,\n",
    "        retrieval_distance: int = 1,\n",
    "        similarity_threshold: float = 0.6,\n",
    "        base_dir='data_repos/ftlr/datasets',\n",
    "        chroma_db_dir='indices',\n",
    "        solutions_file='solution_links_english.txt',\n",
    "        call_graph_file='method_callgraph.json',\n",
    "        all_code_files_path='all_code_filenames.txt',\n",
    "        all_req_file_names='all_req_filenames.txt',\n",
    "    ):\n",
    "\n",
    "        self.sem_evaluator = SemanticSimilarityEvaluator(\n",
    "            embed_model=Settings.embed_model,\n",
    "            similarity_threshold=similarity_threshold,\n",
    "        )\n",
    "        self.retrieval_distance = retrieval_distance\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        self.dataset_name = dataset_name\n",
    "\n",
    "        self.all_code_files_path = all_code_files_path\n",
    "        self.all_req_file_names = all_req_file_names\n",
    "\n",
    "        self.base_dir = base_dir\n",
    "        self.dataset_dir = f'{base_dir}/{dataset_name}'\n",
    "        indices_path = f\"{chroma_db_dir}/{dataset_name}\"\n",
    "        os.makedirs(indices_path, exist_ok=True)\n",
    "\n",
    "        self.solutions_file_path = f'{self.dataset_dir}/{dataset_name.lower()}_{solutions_file}'\n",
    "        self.call_graph_file = f'{dataset_name.lower()}_{call_graph_file}'\n",
    "        self.class_names2node_map = None\n",
    "    \n",
    "\n",
    "    def set_dataset_data(\n",
    "            self, \n",
    "            use_mc=False, \n",
    "            use_summary=False,\n",
    "            use_similar_q=False\n",
    "        ):\n",
    "        self.use_mc = use_mc\n",
    "        self.use_summary = use_summary\n",
    "        self.use_similar_q = use_similar_q\n",
    "        \n",
    "        indexed_nodes, req_nodes = get_dataset_nodes(\n",
    "            self.dataset_name, \n",
    "            'indexed_nodes',\n",
    "            use_mc=use_mc,\n",
    "            use_summary=use_summary\n",
    "        )\n",
    "\n",
    "        if self.use_similar_q:\n",
    "            for i in range(len(req_nodes)):\n",
    "                similar_qs = '\\n'.join(req_nodes[i].metadata['similar_queries'][:2])\n",
    "                req_nodes[i].text += f\"\\n\\nSet of similar requirements as context: \\n{similar_qs}\"\n",
    "\n",
    "        self.indexed_nodes = indexed_nodes\n",
    "        self.req_nodes = req_nodes\n",
    "        \n",
    "        self.set_class_node_maps()\n",
    "        self.set_docs_nxg_and_graph_nodes()\n",
    "        self.set_solution_links()\n",
    "    \n",
    "\n",
    "    def set_solution_links(self):\n",
    "        solutions = get_solutions(self.solutions_file_path)\n",
    "        self.req_file_to_node_map = {\n",
    "            n.metadata['file_name']: solutions[n.metadata['file_name']]\n",
    "            for n in self.req_nodes\n",
    "        }\n",
    "    \n",
    "\n",
    "    def set_class_node_maps(self):\n",
    "        self.node_map = {n.hash: n for n in self.indexed_nodes}\n",
    "        self.class_names2node_map = defaultdict(list)\n",
    "        for n in self.indexed_nodes:\n",
    "            self.class_names2node_map[n.metadata[CLASS_NAME_LABEL]].append(n.hash)\n",
    "\n",
    "\n",
    "    def set_docs_nxg_and_graph_nodes(self):\n",
    "        self.docs_nxg, self.graph_class_nodes = get_docs_nxg(\n",
    "            dataset_dir=self.dataset_dir,\n",
    "            all_code_files_path=self.all_code_files_path,\n",
    "            callgraph_file_name=self.call_graph_file\n",
    "        )\n",
    "    \n",
    "    async def get_semantic_score(self, query_str, doc_str):\n",
    "        # print(self.sem_evaluator._embed_model)\n",
    "        result = await self.sem_evaluator.aevaluate(response=doc_str, reference=query_str)\n",
    "        return result\n",
    "\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        # print(\"Query: \", query_bundle.query_str)\n",
    "        retrieved_nodes: List[NodeWithScore] = []\n",
    "        for retriever in self.retrievers:\n",
    "            retrieved_nodes.extend(retriever.retrieve(query_bundle.query_str))\n",
    "        \n",
    "        retrieved_nodes = list({n.node_id: n for n in retrieved_nodes}.values())\n",
    "        \n",
    "        class_names = list(set([n.metadata['Class Name'] for n in retrieved_nodes]))\n",
    "        assert all(n in self.docs_nxg for n in class_names)\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"Class names\", class_names)\n",
    "\n",
    "        reachable_classes = list(\n",
    "            set(sum([\n",
    "                get_reachable_nodes(self.docs_nxg, class_name, self.retrieval_distance)\\\n",
    "                for class_name in class_names], []\n",
    "            ))\n",
    "        )\n",
    "        node_with_scores = list()\n",
    "        for c in reachable_classes:\n",
    "            if c not in self.graph_class_nodes:\n",
    "                continue\n",
    "            class_node_str = get_graph_node_str(self.graph_class_nodes[c])\n",
    "            doc = self.node_map[self.class_names2node_map[c][0]]\n",
    "            if 'section_summary' in doc.metadata:\n",
    "                class_node_str += f\"\\n\\nSummary: {doc.metadata['section_summary']}\"\n",
    "\n",
    "            node = Document(\n",
    "                text=class_node_str, \n",
    "                metadata=doc.metadata,\n",
    "                excluded_llm_metadata_keys=doc.excluded_llm_metadata_keys,\n",
    "                excluded_embed_metadata_keys=doc.excluded_embed_metadata_keys\n",
    "            )\n",
    "            sim_result = asyncio.run(self.get_semantic_score(query_bundle.query_str, class_node_str))\n",
    "            \n",
    "            if sim_result.passing:\n",
    "                node_with_score = NodeWithScore(node=node, score=sim_result.score)\n",
    "                node_with_scores.append(node_with_score)\n",
    "        \n",
    "        # print(\"Retrieved nodes: \", len(node_with_scores))\n",
    "        return node_with_scores\n",
    "\n",
    "\n",
    "    def set_retrivers(\n",
    "        self,\n",
    "        types: List[str],\n",
    "        similarity_top_k: int = 2,\n",
    "    ):\n",
    "        self.retrievers: Union[BaseRetriever, List[BaseRetriever]] = []\n",
    "        if VECTOR_INDEX_RETREIVER in types:\n",
    "            vector_index = VectorStoreIndex(nodes=self.indexed_nodes, show_progress=True)\n",
    "            vector_retriever = vector_index.as_retriever(similarity_top_k=similarity_top_k)\n",
    "            self.retrievers.append(vector_retriever)\n",
    "        \n",
    "        if BM25_INDEX_RETREIVER in types:\n",
    "            bm25_retriever = BM25Retriever.from_defaults(\n",
    "            nodes=self.indexed_nodes, \n",
    "            similarity_top_k=similarity_top_k,\n",
    "        )\n",
    "            self.retrievers.append(bm25_retriever)\n",
    "    \n",
    "\n",
    "    def evaluate_retrievers(self, retrievers: List[str], both=False):\n",
    "        self.set_retrivers(retrievers)\n",
    "        retrievers_str = '_'.join(retrievers)\n",
    "        \n",
    "        qes = {f'{retrievers_str}_nl': RetrieverQueryEngine(self)}\n",
    "        \n",
    "        if both:\n",
    "            qes[retrievers_str] = RetrieverQueryEngine(self.retrievers[0])\n",
    "        \n",
    "        \n",
    "\n",
    "        correctness_results = evaluate_response(\n",
    "            req_nodes=self.req_nodes,\n",
    "            query_engines=qes,\n",
    "            solutions_file=self.solutions_file_path,\n",
    "            dataset_name=self.dataset_name\n",
    "        )\n",
    "        mc = '_mc' if self.use_mc else ''\n",
    "        sm = '_sm' if self.use_summary else ''\n",
    "        eq = '_eq' if self.use_similar_q else ''\n",
    "        with open(f'results/{self.dataset_name}_{retrievers_str}{mc}{sm}{eq}_results.json', 'w') as f:\n",
    "            json.dump(correctness_results, f, indent=4)\n",
    "\n",
    "\n",
    "    def trace(self):\n",
    "        self.evaluate_retrievers([VECTOR_INDEX_RETREIVER], both=True)\n",
    "        self.evaluate_retrievers([BM25_INDEX_RETREIVER], both=True)\n",
    "        self.evaluate_retrievers([VECTOR_INDEX_RETREIVER, BM25_INDEX_RETREIVER])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datase_name = 'smos'\n",
    "use_mc = False\n",
    "use_summary = False\n",
    "use_similar_q = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of indexed nodes: 395\n",
      "Number of requirements nodes: 67\n",
      "Extracting class info objects\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dcebb4664994c559cc6f0045a57b273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in the graph: 543\n",
      "Extracting call graph links\n",
      "Total: 2354\n",
      "Present: 2354, Absent: 0\n",
      "Number of nodes in the graph: 599\n",
      "Adding method calls\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7273cf0c854a42258494a578fac9b00e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating Documents:   0%|          | 0/599 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "081584368f9e482c802a5a39b289774a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a3f65435f14bdabb8dcf4d23155d98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating Networkx Graph:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nl2code_tracer = NL2CodeTracer(datase_name)\n",
    "nl2code_tracer.set_dataset_data(use_mc=use_mc, use_summary=use_summary, use_similar_q=use_similar_q)\n",
    "# nl2code_tracer.trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What are the names of the classes that are related to the following use case requirement?\n",
      "\n",
      "Nome: Login\n",
      "Attori: Utente registrato\n",
      "Descrizione: Permette ad un utente di loggarsi al sistema\n",
      "Precondizioni:\n",
      "L’utente non è loggato al sistema L’utente possiede username e password Compila e sottomette il form per il login\n",
      "Sequenza degli eventi\n",
      "Utente\n",
      "Sistema\n",
      "1.\tVerifica che username e password abbiano lunghezza >=5. Se la condizione è rispettata passa al punto due, altrimenti notifica l'errore all'utente.\n",
      "2.\tCerca nell’archivio se username e password inseriti dall’utente sono presenti tra gli utenti loggabili\n",
      "3.\tSe la ricerca ha avuto successo l’utente viene loggato al sistema\n",
      "Postcondizioni:\n",
      "•\tIl sistema visualizza l’area di lavoro dell’Utente Registrato •\tInterruzione della connessione al server SMOS\n",
      "\n",
      "Set of similar requirements as context: \n",
      "**Requirement 1:** The system shall validate the username and password length to ensure they meet the minimum length requirement of 5 characters before attempting to authenticate the user.\n",
      "**Requirement 2:** The system shall authenticate a registered user by checking the provided username and password against the stored credentials in the database, and grant access to the system if the credentials match.\n",
      "\n",
      "Provide the answer in a list format and provide ONLY the list of class names as a JSON list.\n",
      "[<\"Class 1 Name\">, <\"Class 2 Name\">, ... <\"Class N Name\">] where N can be up to 10.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from prompts.templates import CLASS_TRACE_TEMPLATE\n",
    "\n",
    "\n",
    "q = CLASS_TRACE_TEMPLATE.format(requirement=nl2code_tracer.req_nodes[0].text)\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168d5b1ca651438994e62976ef7b43b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nl2code_tracer.set_retrivers([VECTOR_INDEX_RETREIVER])\n",
    "retrieved_nodes: List[NodeWithScore] = []\n",
    "for retriever in nl2code_tracer.retrievers:\n",
    "    retrieved_nodes.extend(retriever.retrieve(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl2code_tracer.set_retrivers([BM25_INDEX_RETREIVER])\n",
    "retrieved_nodes: List[NodeWithScore] = []\n",
    "for retriever in nl2code_tracer.retrievers:\n",
    "    retrieved_nodes.extend(retriever.retrieve(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ConnessioneWrapper', 'Utility']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set([n.metadata['Class Name'] for n in retrieved_nodes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['User',\n",
       " 'Role',\n",
       " 'UserListItem',\n",
       " 'managerUser',\n",
       " 'ServletLogin',\n",
       " 'ServletLogout',\n",
       " 'ServletAlterPersonalDate']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nl2code_tracer.req_file_to_node_map[nl2code_tracer.req_nodes[0].metadata['file_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8459037741813271"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_result = await nl2code_tracer.get_semantic_score(q, nl2code_tracer.req_nodes[0].text)\n",
    "sim_result.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nl2code_tracer.sem_evaluator._embed_model.get_text_embedding(CLASS_TRACE_TEMPLATE.format(requirement=nl2code_tracer.req_nodes[1].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:  67\n",
      "Before 67\n",
      "After 0\n",
      "Total:  58\n",
      "Before 58\n",
      "After 0\n",
      "Total:  131\n",
      "Before 131\n",
      "After 0\n",
      "Total:  139\n",
      "Before 139\n",
      "After 0\n"
     ]
    }
   ],
   "source": [
    "datasets = ['smos', 'eTour', 'iTrust', 'eANCI']\n",
    "for dataset in datasets:\n",
    "    with open(f'similar_requirements/{dataset}.pkl', 'rb') as f:\n",
    "        reqs = pickle.load(f)\n",
    "\n",
    "    print(\"Total: \", len(reqs))\n",
    "    print(\"Before\", len([1 for req in reqs if \", \".join(req.metadata['similar_queries']).startswith('Here are')]))\n",
    "    for req in reqs:\n",
    "        if \", \".join(req.metadata['similar_queries']).startswith('Here are'):\n",
    "            req.metadata['similar_queries'] = req.metadata['similar_queries'][1:]\n",
    "\n",
    "    with open(f'similar_requirements/{dataset}.pkl', 'wb') as f:\n",
    "        pickle.dump(reqs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LLM: meta-llama/Meta-Llama-3-70B-Instruct\n",
      "Using Embedding Model: BAAI/bge-m3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junaid/Anaconda/anaconda3/envs/ML/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/junaid/Anaconda/anaconda3/envs/ML/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of indexed nodes: 897\n",
      "Number of requirements nodes: 67\n",
      "Extracting class info objects\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf88cd3f941b46e2ac3406a0bf710187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in the graph: 543\n",
      "Extracting call graph links\n",
      "Total: 2354\n",
      "Present: 2354, Absent: 0\n",
      "Number of nodes in the graph: 599\n",
      "Adding method calls\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6432603166cd4fc9abfc81f05accf0e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating Documents:   0%|          | 0/599 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001603eb218e49e89370505721349e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c8869248179442eb212d7c94ae5eb45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating Networkx Graph:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c4d74fd16114cbf954fb2330f9fed73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating for vector_index_retriever_nl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/67 [00:00<?, ?Requirement/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from parameters import parse_args\n",
    "import os\n",
    "from constants import (\n",
    "    LLMsMap,\n",
    "    EmbeddingModelsMap,\n",
    ")\n",
    "\n",
    "from api_models import set_llm_and_embed\n",
    "args = parse_args()\n",
    "\n",
    "datasets = [\n",
    "    ('smos', 'bge_m3'), \n",
    "    ('eTour', 'bge_large'), \n",
    "    ('eANCI' 'bge_m3'), \n",
    "    ('iTrust', 'bge_large')\n",
    "]\n",
    "use_mcs = [True, False]\n",
    "use_summaries = [True, False]\n",
    "use_similar_qs = [True, False]\n",
    "\n",
    "for dataset, embed_model in datasets:\n",
    "    llm_name = LLMsMap[args.llm]\n",
    "    embed_model_name = EmbeddingModelsMap[embed_model]\n",
    "    print(f\"Using LLM: {llm_name}\")\n",
    "    print(f\"Using Embedding Model: {embed_model_name}\")\n",
    "\n",
    "    set_llm_and_embed(\n",
    "        llm_type=args.llm_type,\n",
    "        llm_name=llm_name,\n",
    "        embed_model_name=embed_model_name,\n",
    "    )\n",
    "    configs = [(mc, sm, eq) for mc in use_mcs for sm in use_summaries for eq in use_similar_qs]\n",
    "    for config in configs:\n",
    "        use_mc, use_summary, use_similar_q = config\n",
    "        nl2code_tracer = NL2CodeTracer(dataset)\n",
    "        nl2code_tracer.set_dataset_data(\n",
    "            use_mc=use_mc, \n",
    "            use_summary=use_summary, \n",
    "            use_similar_q=use_similar_q\n",
    "        )\n",
    "        nl2code_tracer.trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
